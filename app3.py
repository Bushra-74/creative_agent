import streamlit as st
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
@st.cache_resource
def load_model():
    tokenizer = AutoTokenizer.from_pretrained("akhooli/gpt2-small-arabic")
    model = AutoModelForCausalLM.from_pretrained("akhooli/gpt2-small-arabic")
    return tokenizer, model

tokenizer, model = load_model()

# ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
st.set_page_config(page_title="ğŸ¯ Creative Content Agent", layout="centered")
st.title("ğŸ¯ Creative Content Agent")

prompt = st.text_area("Ø£Ø¯Ø®Ù„ Ø¨Ø¯Ø§ÙŠØ© Ø§Ù„Ù†Øµ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©", placeholder="Ù…Ø«Ø§Ù„: ÙÙŠ Ø¹Ø§Ù„Ù… Ø§Ù„ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§ Ø§Ù„Ø­Ø¯ÙŠØ«Ø©ØŒ")

if st.button("ğŸ”® ØªÙˆÙ„ÙŠØ¯"):
    if prompt.strip():
        st.info("â³ Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªÙˆÙ„ÙŠØ¯...")
        input_ids = tokenizer.encode(prompt, return_tensors="pt")
        with torch.no_grad():
            output_ids = model.generate(input_ids, max_new_tokens=100, do_sample=True, top_k=50, top_p=0.95)
        output = tokenizer.decode(output_ids[0], skip_special_tokens=True)
        st.success("ğŸ‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù†Ø§ØªØ¬:")
        st.write(output)
    else:
        st.warning("âš ï¸ ÙŠØ±Ø¬Ù‰ Ø¥Ø¯Ø®Ø§Ù„ Ù†Øµ Ù„Ù„Ø¨Ø¯Ø¡.")
